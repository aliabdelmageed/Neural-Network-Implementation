{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    A = np.maximum(0,Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) \n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) \n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) \n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) \n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (209, 64, 64, 3)\n",
      "train_y shape: (1, 209)\n",
      "test_x_orig shape: (50, 64, 64, 3)\n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "n_x = train_x.shape[0]\n",
    "layers_dims = [n_x, 20, 7, 5, 1] #  4-layer model\n",
    "# Standardization\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "# Explore dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in the network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network \"including input\"\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "     \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        W = parameters[\"W\"+str(l)]\n",
    "        b = parameters[\"b\"+str(l)]\n",
    "        Z = np.dot(W,A_prev)+b\n",
    "        linear_cache = (A_prev, W, b)\n",
    "        # Relu\n",
    "        A, activation_cache = relu(Z)\n",
    "        assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    W_last = parameters[\"W\"+str(L)]\n",
    "    b_last = parameters[\"b\"+str(L)]\n",
    "    Z = np.dot(W_last,A)+b_last\n",
    "    linear_cache = (A, W_last, b_last)\n",
    "    # Sigmoid\n",
    "    A_last, activation_cache = sigmoid(Z)\n",
    "    assert (A_last.shape == (W_last.shape[0], A.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(A_last.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return A_last, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A_last, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function\n",
    "\n",
    "    Arguments:\n",
    "    A_last -- probability vector corresponding to label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Compute loss from aL and y.\n",
    "    cost = (-1/m)* np.sum(np.dot(Y,np.log(A_last).T)+np.dot((1-Y),np.log(1-A_last).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(A_last, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    A_last -- probability vector, output of the forward propagation\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation with \"sigmoid\"(it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = A_last.shape[1]\n",
    "    Y = Y.reshape(A_last.shape) # after this line, Y is the same shape as A_last\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, A_last) - np.divide(1 - Y, 1 - A_last))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[-1]\n",
    "    linear_cache, activation_cache = current_cache\n",
    "    # sigmoid backward\n",
    "    Z = activation_cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dAL * s * (1-s)\n",
    "    \n",
    "    # linear backward\n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "        \n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = dA_prev, dW, db\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        linear_cache, activation_cache = current_cache\n",
    "        # relu_backward\n",
    "        Z = activation_cache\n",
    "        dZ = np.array(grads[\"dA\" + str(l + 2)], copy=True) # just converting dz to a correct object.\n",
    "        # When z <= 0, dz should be 0. \n",
    "        dZ[Z <= 0] = 0\n",
    "        assert (dZ.shape == Z.shape)\n",
    "        \n",
    "        # linear_backward\n",
    "        A_prev, W, b = linear_cache\n",
    "        m = A_prev.shape[1]\n",
    "        dW_temp = (1/m)*np.dot(dZ,A_prev.T)\n",
    "        db_temp = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "        dA_prev_temp = np.dot(W.T,dZ)\n",
    "\n",
    "        assert (dA_prev_temp.shape == A_prev.shape)\n",
    "        assert (dW_temp.shape == W.shape)\n",
    "        assert (db_temp.shape == b.shape)\n",
    "\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing gradients\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X, Y, layers_dims, learning_rate = 0.0075, epochs = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a deep neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters(layers_dims)    \n",
    "    \n",
    "    # gradient descent\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        A_last, caches = forward_prop(X,parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(A_last, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = backward_prop(A_last, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        model = update_parameters(parameters, grads, learning_rate=learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.771749\n",
      "Cost after iteration 100: 0.672053\n",
      "Cost after iteration 200: 0.648263\n",
      "Cost after iteration 300: 0.611507\n",
      "Cost after iteration 400: 0.567047\n",
      "Cost after iteration 500: 0.540138\n",
      "Cost after iteration 600: 0.527930\n",
      "Cost after iteration 700: 0.465477\n",
      "Cost after iteration 800: 0.369126\n",
      "Cost after iteration 900: 0.391747\n",
      "Cost after iteration 1000: 0.315187\n",
      "Cost after iteration 1100: 0.272700\n",
      "Cost after iteration 1200: 0.237419\n",
      "Cost after iteration 1300: 0.199601\n",
      "Cost after iteration 1400: 0.189263\n",
      "Cost after iteration 1500: 0.161189\n",
      "Cost after iteration 1600: 0.148214\n",
      "Cost after iteration 1700: 0.137775\n",
      "Cost after iteration 1800: 0.129740\n",
      "Cost after iteration 1900: 0.121225\n",
      "Cost after iteration 2000: 0.113821\n",
      "Cost after iteration 2100: 0.107839\n",
      "Cost after iteration 2200: 0.102855\n",
      "Cost after iteration 2300: 0.100897\n",
      "Cost after iteration 2400: 0.092878\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19d711302e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(train_x, train_y, layers_dims, epochs = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
